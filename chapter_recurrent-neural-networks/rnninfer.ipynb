{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import gluonbook as gb\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, nn, rnn, utils as gutils\n",
    "import numpy as np\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('../data/ptb.zip', 'r') as zin:\n",
    "    zin.extractall('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word_to_idx:\n",
    "            self.idx_to_word.append(word)\n",
    "            self.word_to_idx[word] = len(self.idx_to_word) - 1\n",
    "        return self.word_to_idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(path + 'train.txt')\n",
    "        self.valid = self.tokenize(path + 'valid.txt')\n",
    "        self.test = self.tokenize(path + 'test.txt')\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        # 将词语添加至词典。\n",
    "        with open(path, 'r') as f:\n",
    "            num_words = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                num_words += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "        \n",
    "        with open(path, 'r') as f:\n",
    "            indices = np.zeros((num_words,), dtype='int32')\n",
    "            idx = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    indices[idx] = self.dictionary.word_to_idx[word]\n",
    "                    idx += 1\n",
    "        return nd.array(indices, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building data done, dic size is 10000\n"
     ]
    }
   ],
   "source": [
    "data = '../data/ptb/ptb.'\n",
    "corpus = Corpus(data)\n",
    "vocab_size = len(corpus.dictionary)\n",
    "print(\"building data done, dic size is %d\"%vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Block):\n",
    "    def __init__(self, mode, vocab_size, embed_size, num_hiddens,\n",
    "                 num_layers, drop_prob=0.5, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.dropout = nn.Dropout(drop_prob)\n",
    "            \n",
    "            self.embedding = nn.Embedding(\n",
    "                vocab_size, embed_size, weight_initializer=init.Uniform(0.1))\n",
    "            if mode == 'rnn_relu':\n",
    "                self.rnn = rnn.RNN(num_hiddens, num_layers, activation='relu',\n",
    "                                   dropout=drop_prob, input_size=embed_size)\n",
    "            elif mode == 'rnn_tanh':\n",
    "                self.rnn = rnn.RNN(num_hiddens, num_layers, activation='tanh',\n",
    "                                   dropout=drop_prob, input_size=embed_size)\n",
    "            elif mode == 'lstm':\n",
    "                self.rnn = rnn.LSTM(num_hiddens, num_layers,\n",
    "                                    dropout=drop_prob, input_size=embed_size)\n",
    "            elif mode == 'gru':\n",
    "                self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=drop_prob,\n",
    "                                   input_size=embed_size)\n",
    "            else:\n",
    "                raise ValueError('Invalid mode %s. Options are rnn_relu, '\n",
    "                                 'rnn_tanh, lstm, and gru' % mode)\n",
    "            self.dense = nn.Dense(vocab_size, in_units=num_hiddens)\n",
    "            self.num_hiddens = num_hiddens\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        embedding = self.dropout(self.embedding(inputs))\n",
    "        output, state = self.rnn(embedding, state)\n",
    "        output = self.dropout(output)\n",
    "        output = self.dense(output.reshape((-1, self.num_hiddens)))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)\n",
    "    \n",
    "    def save_model(self, model_path):\n",
    "        self.rnn.save_parameters(model_path)\n",
    "        \n",
    "    def restore_model(self, model_path, ctx):\n",
    "        self.rnn.load_parameters(model_path,ctx)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'rnn_relu'\n",
    "embed_size = 100\n",
    "num_hiddens = 100\n",
    "num_layers = 2\n",
    "lr = 0.5\n",
    "clipping_theta = 0.2\n",
    "num_epochs = 2\n",
    "batch_size = 32\n",
    "num_steps = 5\n",
    "drop_prob = 0.2\n",
    "eval_period = 1000\n",
    "\n",
    "ctx = gb.try_gpu()\n",
    "model = RNNModel(model_name, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 drop_prob)\n",
    "model.initialize(init.Xavier(), ctx=ctx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "def batchify(data, batch_size):\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    data = data[: num_batches * batch_size]\n",
    "    data = data.reshape((batch_size, num_batches)).T\n",
    "    return data\n",
    "\n",
    "train_data = batchify(corpus.train, batch_size).as_in_context(ctx)\n",
    "val_data = batchify(corpus.valid, batch_size).as_in_context(ctx)\n",
    "test_data = batchify(corpus.test, batch_size).as_in_context(ctx)\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(num_steps, source.shape[0] - 1 - i)\n",
    "    X = source[i : i + seq_len]\n",
    "    Y = source[i + 1 : i + 1 + seq_len]\n",
    "    return X, Y.reshape((-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "def eval_rnn(data_source):\n",
    "    durations=[]\n",
    "    state = model.begin_state(func=nd.zeros, batch_size=batch_size, ctx=ctx)\n",
    "    for i in range(0, data_source.shape[0] - 1, num_steps):\n",
    "        X, y = get_batch(data_source, i)\n",
    "        begin = time.time()\n",
    "        output, state = model(X, state)\n",
    "        durations.append(time.time()-begin)\n",
    "    timings=np.array(durations)\n",
    "    n = len(timings)\n",
    "    batch_avg = np.mean(timings)*1000\n",
    "    sample_avg = batch_avg/batch_size\n",
    "    print('infer %d times with batch size %d,  %.8f mean for per batch, %.8f for per sample'% (n, batch_size, batch_avg, sample_avg))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [],
   "source": [
    "model_file_path=\"../checkpoints/rnn.params\"\n",
    "#model.save_model(model_file_path)\n",
    "model.restore_model(model_file_path, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infer 515 times with batch size 32,  0.51779191 mean for per batch, 0.01618100 for per sample\n"
     ]
    }
   ],
   "source": [
    "eval_rnn(test_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
